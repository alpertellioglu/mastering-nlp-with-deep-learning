{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2079268f-58ee-4fe7-a2e7-e2c87b0b1e9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip3 install datasets\n",
    "# !pip3 install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b45ff0db-f529-43a3-bb98-938401151d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e09dac3d-4e39-43a1-9a63-93e25f46d12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "#TODO: replace the path with your embedding model on your computer \n",
    "word2vec_model_path = '~/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz'\n",
    "word2vec_model = KeyedVectors.load_word2vec_format(word2vec_model_path, binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be1a34c4-19e0-44e5-8319-ebda1c6f0404",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"imdb\", split='train')\n",
    "\n",
    "#Tokenizer\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "# Build vocabulary\n",
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(dataset['text']), specials=[\"<unk>\", \"<pad>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdcb2bc0-df31-4fe0-9df8-d005e2fac4a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c8b4a68-dd41-4719-9526-0888602a6be8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'I saw this film opening weekend in Australia, anticipating with an excellent cast of Ledger, Edgerton, Bloom, Watts and Rush that the definitive story of Ned Kelly would unfold before me. Unfortunately, despite an outstanding performance by Heath Ledger in the lead role, the plot was paper thin....which doesn\\'t inspire me to read \"Our Sunshine\". There were some other plus points, the support acting from Edgerton in particular, assured direction from Jordan (confirming his talent on show in Buffalo Soldiers as well), and production design that gave a real feel of harshness to the Australian bush, much as the Irish immigrants of the early 19th century must have seen it. But I can\\'t help feeling that another opportunity has been missed to tell the real story of an Australian folk hero (or was he?)....in what I suspect is a concession to Hollywood and selling the picture in the US. Oh well, at least Jordan and the producers didn\\'t agree to lose the beards just to please Universal...<br /><br />Guess I will just have to content myself with Peter Carey\\'s excellent \"Secret History of the Kelly Gang\". 4/10',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e3aa37f5-9381-4e2a-af05-7dda54a84b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 300  # Word2Vec vector size\n",
    "weight_matrix = torch.zeros(len(vocab), embedding_dim)\n",
    "\n",
    "for word, index in vocab.get_stoi().items():\n",
    "    try:\n",
    "        word_vector = word2vec_model[word]\n",
    "        weight_matrix[index] = torch.from_numpy(word_vector).clone()\n",
    "    except KeyError:\n",
    "        # For words not in Word2Vec, initialize randomly\n",
    "        weight_matrix[index] = torch.randn(embedding_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "13ccfb6d-6eb0-428f-9dbc-402242649b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Sentiment Model\n",
    "class SentimentModel(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, output_dim, vocab_size, weight_matrix):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding.load_state_dict({'weight': weight_matrix})\n",
    "        self.embedding.weight.requires_grad = True\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        _, (hidden, _) = self.lstm(embedded)\n",
    "        return self.fc(hidden.squeeze(0))\n",
    "\n",
    "# Prepare DataLoader\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list = [], []\n",
    "    for _text, _label in batch:\n",
    "        processed_text = [vocab[token] for token in tokenizer(_text)]\n",
    "        label_list.append(_label)\n",
    "        text_list.append(torch.tensor(processed_text, dtype=torch.int64))\n",
    "    return pad_sequence(text_list, padding_value=vocab[\"<pad>\"]), torch.tensor(label_list, dtype=torch.float32)\n",
    "\n",
    "train_dataset = list(zip(dataset['text'], dataset['label']))\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "model = SentimentModel(embedding_dim, 256, 1, len(vocab), weight_matrix)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.BCEWithLogitsLoss() #loss function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "de03165e-8985-4de1-9e78-ace680b83ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Average Loss: 0.6925777795410156\n",
      "Epoch 2, Average Loss: 0.6794356470489502\n",
      "Epoch 3, Average Loss: 0.39065737891197205\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for texts, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(texts).squeeze(1)\n",
    "        loss = criterion(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Average Loss: {total_loss / len(train_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "25bab701-4019-48a7-9b05-2910f7436771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Analysis: Positive\n"
     ]
    }
   ],
   "source": [
    "def predict_sentiment(model, sentence):\n",
    "    model.eval() \n",
    "    tokens = [vocab[token] for token in tokenizer(sentence)]\n",
    "    padded_tokens = pad_sequence([torch.tensor(tokens, dtype=torch.int64)], padding_value=vocab[\"<pad>\"])\n",
    "    with torch.no_grad():\n",
    "        prediction = torch.sigmoid(model(padded_tokens))\n",
    "    return prediction.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d5791c2f-92f9-4a97-aebe-96f83323b232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n",
      "0.8908994197845459\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "sentence = \"This is the best movie!\"\n",
    "prediction = predict_sentiment(model, sentence)\n",
    "print('Positive' if prediction > 0.5 else 'Negative')\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ca0284cb-ec4a-4580-887a-5fb2f4ce6177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative\n",
      "0.036396417766809464\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "sentence = \"Worst thing I've watched\"\n",
    "prediction = predict_sentiment(model, sentence)\n",
    "print('Positive' if prediction > 0.5 else 'Negative')\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2798d7bd-ce22-447e-8461-f4a777ee8e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n",
      "0.7404760718345642\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "sentence = \"I don't think this is a good movie\"\n",
    "prediction = predict_sentiment(model, sentence)\n",
    "print('Positive' if prediction > 0.5 else 'Negative')\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74761b8f-1e80-42ba-8082-b1eb1ece7224",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
